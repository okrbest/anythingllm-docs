---
title: "로컬 Docker 설치"
description: "TeamplGPT의 로컬 Docker 설치 가이드"
---

import { Callout, Tabs } from "nextra/components";
import Image from "next/image";

<Image
  src="/images/getting-started/installation/local-docker/header-image.png"
  height={1080}
  width={1920}
  quality={100}
  alt="TeamplGPT Installation Local Docker"
/>

# Dockerized TeamplGPT 사용 방법

Dockerized 버전의 TeamplGPT를 사용하여 훨씬 빠르고 완전한 TeamplGPT 시작을 경험하세요.

## 최소 요구 사항

<Callout type="info" emoji="️💡">
  **팁** ➤➤ AWS/GCP/Azure에서 TeamplGPT를 실행하시나요?

➤ 최소 2GB의 RAM을 목표로 해야 합니다. 디스크 저장소는 저장할 데이터(문서, 벡터, 모델 등)의 양에 비례합니다. 최소 10GB 권장.

</Callout>

- `docker`가 장치에 설치되어 있어야 합니다.
- `yarn`과 `node`가 장치에 설치되어 있어야 합니다.
- 로컬 또는 원격으로 실행 중인 LLM에 접근할 수 있어야 합니다.

<Callout type="info" emoji="️💡">
  **참고**

➤ TeamplGPT는 기본적으로 [LanceDB](https://github.com/lancedb/lancedb)가 제공하는 내장 벡터 데이터베이스를 사용합니다.

➤ TeamplGPT는 기본적으로 인스턴스에서 텍스트를 개인적으로 임베드합니다 [더 알아보기](../server/storage/models/README.md)

</Callout>

## Dockerized TeamplGPT 실행 권장 방법!

<Callout type="warning" emoji="️⚠️">
  **중요!**

➤ localhost에서 Chroma, LocalAi 또는 LMStudio와 같은 다른 서비스를 실행 중인 경우, Docker 컨테이너 내에서 TeamplGPT를 사용할 때 `localhost:xxxx`가 호스트 시스템에 대해 해결되지 않으므로 `http://host.docker.internal:xxxx`를 사용해야 합니다.

➤ **필수** Docker v18.03+ (Win/Mac) 및 20.10+ (Linux/Ubuntu) 버전에서는 host.docker.internal이 해결됩니다!

➤ _Linux_: Docker 실행 명령에 `--add-host=host.docker.internal:host-gateway`를 추가하여 이를 해결하십시오.

➤ 예: 호스트 머신에서 localhost:8000에서 실행 중인 Chroma 호스트 URL은 TeamplGPT에서 `http://host.docker.internal:8000`으로 지정해야 합니다.

</Callout>

<Callout type="info" emoji="️💡">
  **팁** ➤➤ 컨테이너의 저장소 볼륨을 호스트 머신의 폴더에 마운트하면 향후 업데이트를 가져올 때 기존 데이터를 삭제하지 않고 사용할 수 있습니다.
</Callout>

최신 이미지를 Docker에서 가져옵니다. `amd64` 및 `arm64` CPU 아키텍처를 모두 지원합니다.

```shell
 docker pull mintplexlabs/anythingllm
```

저장소를 로컬에 마운트하고 Docker에서 TeamplGPT를 실행합니다.

<Tabs items={['Linux/Mac', 'Windows']} defaultIndex="0">
  <Tabs.Tab>
  ```shell copy showLineNumbers
  export STORAGE_LOCATION=$HOME/anythingllm && \
  mkdir -p $STORAGE_LOCATION && \
  touch "$STORAGE_LOCATION/.env" && \
  docker run -d -p 3001:3001 \
  --cap-add SYS_ADMIN \
  -v ${STORAGE_LOCATION}:/app/server/storage \
  -v ${STORAGE_LOCATION}/.env:/app/server/.env \
  -e STORAGE_DIR="/app/server/storage" \
  mintplexlabs/anythingllm
  ```
 </Tabs.Tab>

 <Tabs.Tab>
  ```powershell copy showLineNumbers
  $env:STORAGE_LOCATION="$HOME\Documents\anythingllm"; `
  If(!(Test-Path $env:STORAGE_LOCATION)) {New-Item $env:STORAGE_LOCATION -ItemType Directory}; `
  If(!(Test-Path "$env:STORAGE_LOCATION\.env")) {New-Item "$env:STORAGE_LOCATION\.env" -ItemType File}; `
  docker run -d -p 3001:3001 `
  --cap-add SYS_ADMIN `
  -v "$env:STORAGE_LOCATION`:/app/server/storage" `
  -v "$env:STORAGE_LOCATION\.env:/app/server/.env" `
  -e STORAGE_DIR="/app/server/storage" `
  mintplexlabs/anythingllm;
  ```
 </Tabs.Tab>
</Tabs>

`http://localhost:3001`로 이동하면 이제 TeamplGPT를 사용할 수 있습니다! 모든 데이터와 진행 상황은 컨테이너 재빌드 또는 Docker Hub에서 가져오기 간에 지속됩니다.

## 사용자 인터페이스 사용 방법

전체 애플리케이션에 액세스하려면 브라우저에서 `http://localhost:3001`로 이동하세요.

## ENV의 UID 및 GID 정보

- UID 및 GID는 기본적으로 1000으로 설정됩니다. 이는 Docker 컨테이너 및 대부분의 호스트 운영 체제의 기본 사용자입니다.
- 호스트 사용자 UID 및 GID와 `.env` 파일에 설정된 값 간에 불일치가 있는 경우 권한 문제가 발생할 수 있습니다.

## 소스에서 로컬로 빌드 (일반 사용자는 권장하지 않음)

- 이 저장소를 `git clone`하고 루트 디렉토리로 이동합니다.
- 빈 SQLite DB 파일을 생성하려면 `touch server/storage/teamplgpt.db`를 실행합니다.
- `docker/`로 이동합니다.
- `cp .env.example .env` **빌드하기 전에 반드시 실행해야 합니다**
- 이미지를 빌드하려면 `docker-compose up -d --build`를 실행합니다. 이 작업은 몇 분 정도 소요됩니다.

빌드 프로세스가 완료되면 Docker 호스트가 이미지를 온라인 상태로 표시합니다. 이 애플리케이션은 `http://localhost:3001`에서 빌드됩니다.

## 통합 및 원클릭 설정

아래 통합은 커뮤니티에서 만들어진 템플릿이나 도구로, TeamplGPT의 Docker 경험을 더 쉽게 만듭니다.

### Midori AI Subsystem을 사용하여 TeamplGPT 관리

<Callout type="warning" emoji="️💡">
  **참고!** ➤➤ Midori AI Subsystem Manager는 현재 BETA 상태입니다. Subsystem Manager 사용 중 문제가 발생하면 [여기](https://io.midori-ai.xyz/about-us/contact-us/)로 연락해 주세요.

_Midori AI Subsystem Manager는 **Mintplex Labs에서 유지 관리하지 않으며**, 커뮤니티가 주도하는 프로젝트입니다. 따라서 이 메시지를 사용하는 데 문제가 있는 경우 위 링크의 디스코드 링크로 연락해 주세요._

</Callout>

호스트 OS에 대한 설정은 [Midori AI Subsystem 가이드](https://io.midori-ai.xyz/subsystem/manager/)를 따르세요.

설정을 완료한 후, Midori AI Subsystem에 TeamplGPT Docker 백엔드를 설치합니다.

설치가 완료되면 모든 준비가 완료됩니다!

## 자주 묻는 질문 및 해결 방법

### 1. localhost에서 실행 중인 서비스에 연결할 수 없습니다!

Docker에서 로컬 인터페이스나 루프백에서 실행 중인 호스트 머신에서 실행 중인 서비스에 연결할 수 없는 경우:

- `localhost`
- `127.0.0.1`
- `0.0.0.0`

<Callout type="warning" emoji="️⚠️">
  **중요!**

➤ Linux에서는 `http://host.docker.internal:xxxx`가 작동하지 않습니다.

➤ 대신 `http://172.17.0.1:xxxx`를 사용하여 이 기능을 에뮬레이트하십시오.

</Callout>

그런 다음 Docker에서는 localhost 부분을 `host.docker.internal`로 교체해야 합니다. 예를 들어, 호스트 머신에서 `http://127.0.0.1:11434`에 바인딩된 Ollama를 실행하는 경우 TeamplGPT의 연결 URL에 `http://host.docker.internal:11434`를 입력해야 합니다.

### 2. API가 작동하지 않거나, 로그인할 수 없거나, LLM이 "오프라인" 상태입니다?

Docker 컨테이너를 EC2와 같은 원격 머신 또는 다른 인스턴스에서 실행 중이고 접근 가능한 URL이 `http://localhost:3001`이 아니고 `http://193.xx.xx.xx:3001`과 같은 경우, `docker-compose up -d --build`를 실행하기 전에 `frontend/.env.production`에 다음을 추가해야 합니다.

```shell copy showLineNumbers
# frontend/.env.production
 GENERATE_SOURCEMAP=false
 VITE_API_BASE="http://<YOUR_REACHABLE_IP_ADDRESS>:3001/api"
```

예를 들어, Docker 인스턴스가 `192.186.1.222`에서 사용 가능한 경우 `VITE_API_BASE`는 `frontend/.env.production`에서 `VITE_API_BASE="http://192.186.1.222:3001/api"`와 같이 설정해야 합니다.

### 3. Ollama와 관련된 문제가 있습니까?

`llama:streaming - could not stream chat. Error: connect ECONNREFUSED 172.17.0.1:11434`와 같은 오류가 발생하면 이 [README](https://github.com/Mintplex-Labs/anything-llm/tree/master/server/utils/AiProviders/ollama)를 참조하세요.

### 여전히 작동하지 않습니까?

Discord [커뮤니티 서버](https://discord.gg/6UyHPeGZAC)에서 도움을 요청하세요.
