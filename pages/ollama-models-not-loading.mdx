---
title: "Why is TeamplGPT not displaying Ollama models?"
description: "We get this question many times a week"
---

import { Callout } from "nextra/components";
import Image from "next/image";

<Image
  src="/images/faq/ollama-models-not-loading/header-image.png"
  height={1080}
  width={1920}
  quality={100}
  alt="TeamplGPT -- Why is TeamplGPT not displaying Ollama models?"
/>

## TeamplGPT Desktop Version

If you are using TeamplGPT desktop version then use `http://127.0.0.1:11434` as Ollama Base URL on TeamplGPT

<Image
  src="/images/faq/ollama-models-not-loading/ollama-correct-url.png"
  height={1080}
  width={1920}
  quality={100}
  alt="Correct Ollama Base URL for TeamplGPT Desktop Version"
/>

## TeamplGPT Docker Version

If you are using TeamplGPT Docker version then use `http://host.docker.internal:11434` as Ollama Base URL on TeamplGPT

<Image
  src="/images/faq/ollama-models-not-loading/ollama-correct-url-docker.png"
  height={1080}
  width={1920}
  quality={100}
  alt="Correct Ollama Base URL for TeamplGPT Docker Version"
/>

<Callout type="info" emoji="ï¸ðŸ’¡">
  **Tip** âž¤âž¤ On linux `http://host.docker.internal:xxxx` does not work. Use
  `http://172.17.0.1:xxxx` instead to emulate this functionality.
</Callout>

## What's the difference between Ollama and the TeamplGPT LLM provider? (Desktop Only)

You can either choose to run Ollama on your own or use the built in TeamplGPT LLM provider. If you use the TeamplGPT provider, a separate instance of Ollama is being run under the hood. This is why if you download models into the version of Ollama that you are running separate from TeamplGPT, it will not show those models as downloaded in the TeamplGPT LLM provider.

<Image
  src="/images/faq/ollama-models-not-loading/anythingllm-ollama-provider.png"
  height={1080}
  width={1920}
  quality={100}
  alt="TeamplGPT built in Ollama provider"
/>

## Why is TeamplGPT not displaying Ollama models?

First make sure ollama is running on the your machine in the background, you can confirm this by visting `http://127.0.0.1:11434`

<Image
  src="/images/faq/ollama-models-not-loading/ollama-running.png"
  height={1080}
  width={1920}
  quality={100}
  alt="Ollama running on background"
/>
