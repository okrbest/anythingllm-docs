---
title: "왜 TeamplGPT가 Ollama 모델을 표시하지 않나요?"
description: "이 질문은 주마다 여러 번 받습니다"
---

import { Callout } from "nextra/components";
import Image from "next/image";

<Image
  src="/images/faq/ollama-models-not-loading/header-image.png"
  height={1080}
  width={1920}
  quality={100}
  alt="TeamplGPT -- 왜 TeamplGPT가 Ollama 모델을 표시하지 않나요?"
/>

## TeamplGPT 데스크탑 버전

TeamplGPT 데스크탑 버전을 사용 중이라면 TeamplGPT의 Ollama 기본 URL로 `http://127.0.0.1:11434`를 사용하세요.

<Image
  src="/images/faq/ollama-models-not-loading/ollama-correct-url.png"
  height={1080}
  width={1920}
  quality={100}
  alt="TeamplGPT 데스크탑 버전의 올바른 Ollama 기본 URL"
/>

## TeamplGPT 도커 버전

TeamplGPT 도커 버전을 사용 중이라면 TeamplGPT의 Ollama 기본 URL로 `http://host.docker.internal:11434`를 사용하세요.

<Image
  src="/images/faq/ollama-models-not-loading/ollama-correct-url-docker.png"
  height={1080}
  width={1920}
  quality={100}
  alt="TeamplGPT 도커 버전의 올바른 Ollama 기본 URL"
/>

<Callout type="info" emoji="️💡">
  **팁** ➤➤ 리눅스에서는 `http://host.docker.internal:xxxx`이 작동하지 않습니다. 대신 `http://172.17.0.1:xxxx`를 사용하여 이 기능을 에뮬레이트하세요.
</Callout>

## Ollama와 TeamplGPT LLM 제공자(데스크탑 전용)의 차이점은 무엇인가요?

Ollama를 직접 실행하거나 내장된 TeamplGPT LLM 제공자를 사용할 수 있습니다. TeamplGPT 제공자를 사용하는 경우, Ollama의 별도 인스턴스가 백그라운드에서 실행됩니다. 그래서 TeamplGPT와 별도로 실행 중인 Ollama 버전에 모델을 다운로드하면, TeamplGPT LLM 제공자에서는 해당 모델이 다운로드된 것으로 표시되지 않습니다.

<Image
  src="/images/faq/ollama-models-not-loading/anythingllm-ollama-provider.png"
  height={1080}
  width={1920}
  quality={100}
  alt="내장된 TeamplGPT Ollama 제공자"
/>

## 왜 TeamplGPT가 Ollama 모델을 표시하지 않나요?

먼저 Ollama가 백그라운드에서 실행 중인지 확인하세요. `http://127.0.0.1:11434`에 접속하여 이를 확인할 수 있습니다.

<Image
  src="/images/faq/ollama-models-not-loading/ollama-running.png"
  height={1080}
  width={1920}
  quality={100}
  alt="백그라운드에서 실행 중인 Ollama"
/>
