---
title: "Ollama 설정 및 문제 해결 가이드"
description: "TeamplGPT와 Ollama 설정을 위한 빠른 해결 방법"
---

import { Callout, Tabs } from "nextra/components";
import Image from "next/image";

<Image
  src="/images/faq/ollama-models-not-loading/header-image.png"
  height={1080}
  width={1920}
  quality={100}
  alt="TeamplGPT와 Ollama 설정 가이드"
/>

# Ollama 연결 문제 해결

## Ollama가 실행 중인지 확인

수정이나 URL 변경을 시도하기 전에 Ollama가 장치에서 제대로 실행되고 있는지 확인하세요:

1. 웹 브라우저를 열고 `http://127.0.0.1:11434`로 이동하세요.
2. 아래와 유사한 페이지가 표시되어야 합니다:

<Image
  src="/images/faq/ollama-models-not-loading/ollama-running.png"
  height={1080}
  width={1920}
  quality={100}
  alt="백그라운드에서 실행 중인 Ollama"
/>

이 페이지가 보이지 않으면 Ollama 설치를 문제를 해결하고 제대로 실행되고 있는지 확인한 후 진행하세요.

## 자동 URL 감지 (LLM & 임베딩 제공자)

<Callout type="info" emoji="ℹ️">
  TeamplGPT는 Ollama에 대한 자동 URL 감지 기능을 제공합니다. 자동 감지가 실패한 경우에만 수동으로 설정이 필요합니다.
</Callout>

### URL 자동 감지 성공
Ollama 제공자를 선택할 때, TeamplGPT는 Ollama URL을 자동으로 감지하려고 합니다. 기본 URL 입력 옵션이 숨겨져 있다면, TeamplGPT가 URL을 자동으로 감지한 것입니다.
<Image
  src="/images/faq/ollama-models-not-loading/ollama-detected-collapsed.png"
  height={1080}
  width={1920}
  quality={100}
  style={{
    "border-radius": "20px",
    "margin-top": "20px",
    "object-fit": "none",
    "object-position": "-320px top",
    width: "100%",
    height: "600px",
  }}
  alt="자동으로 감지된 Ollama URL"
/>

### URL 감지 실패
수동 엔드포인트 입력이 확장된 경우, URL을 감지할 수 없었습니다.
<Image
  src="/images/faq/ollama-models-not-loading/ollama-cannot-detect.png"
  height={1080}
  width={1920}
  quality={100}
  style={{
    "border-radius": "20px",
    "margin-top": "20px",
    "object-fit": "none",
    "object-position": "-320px top",
    width: "100%",
    height: "650px",
  }}
  alt="감지에 실패한 Ollama URL"
/>

TeamplGPT가 URL을 감지하려고 할 때 Ollama가 시작되지 않은 경우, Ollama를 시작한 후 `자동 감지` 버튼을 누르세요. 이렇게 하면 URL을 자동으로 감지하여 `모델` 및 `최대 토큰` 값을 선택할 수 있게 됩니다.

## 올바른 Ollama URL 설정

<Callout type="error" emoji="🚨">
  TeamplGPT가 URL을 자동으로 감지하지 못한 경우, 이는 대부분 Ollama 설정/구성 문제일 가능성이 큽니다. TeamplGPT의 문제가 아닙니다.
</Callout>

Ollama 설치가 제대로 실행되고 있고 방화벽 등에 의해 차단되지 않았음을 100% 확신하는 경우, URL을 수동으로 설정할 수 있습니다.

TeamplGPT 버전을 선택하여 올바른 Ollama URL을 찾으세요:

<Tabs items={['데스크탑', 'Docker', '자체 호스팅']}>
  <Tabs.Tab>
    ### 데스크탑 버전

    사용: `http://127.0.0.1:11434`

    <Image
      src="/images/faq/ollama-models-not-loading/ollama-correct-url.png"
      height={1080}
      width={1920}
      quality={100}
      alt="TeamplGPT 데스크탑 버전에 대한 올바른 Ollama 기본 URL"
    />

  </Tabs.Tab>

  <Tabs.Tab>
    ### Docker 버전

    - Windows/macOS: `http://host.docker.internal:11434`
    - Linux: `http://172.17.0.1:11434`
    <Callout type="warning" emoji="⚠️">
      Linux에서는 `host.docker.internal`이 작동하지 않으므로 `http://172.17.0.1:11434`를 사용하세요.
    </Callout>
    <Image
      src="/images/faq/ollama-models-not-loading/ollama-correct-url-docker.png"
      height={1080}
      width={1920}
      quality={100}
      style={{
        "border-radius": "20px",
        "margin-top": "20px",
        "object-fit": "none",
        "object-position": "-320px top",
        "width": "100%",
        "height": "650px",
      }}
      alt="TeamplGPT Docker 버전에 대한 올바른 Ollama 기본 URL"
    />

  </Tabs.Tab>

  <Tabs.Tab>
    ### 자체 호스팅 버전

    다음 중 하나를 사용하세요:
    - `http://localhost:11434`
    - `http://127.0.0.1:11434`

  </Tabs.Tab>
</Tabs>

## TeamplGPT 데스크탑: 내장형 vs. 독립 실행형 Ollama

TeamplGPT 데스크탑은 두 가지 Ollama 옵션을 제공합니다:

1. **내장형 TeamplGPT LLM 제공자**:
   - 내부적으로 별도의 Ollama 인스턴스를 실행합니다.
   - 독립 실행형 Ollama에 다운로드된 모델은 여기에서 나타나지 않습니다.

2. **독립 실행형 Ollama**:
   - 시스템에서 별도로 Ollama를 실행합니다.
   - URL `http://127.0.0.1:11434`를 사용합니다.

<Image
  src="/images/faq/ollama-models-not-loading/anythingllm-ollama-provider.png"
  height={1080}
  width={1920}
  quality={100}
  alt="내장형 TeamplGPT Ollama 제공자"
/>

## 문제 해결

여전히 문제가 발생하는 경우:

1. 설정에 맞는 올바른 URL을 사용하고 있는지 확인하세요.
2. 연결을 차단하는 방화벽이나 네트워크 문제를 확인하세요.
3. Ollama와 TeamplGPT를 모두 재시작하세요.

<Callout type="info" emoji="💡">
  위의 단계를 시도한 후에도 문제가 지속되면, Discord를 방문하여 질문해 주세요.
</Callout>
