---
title: "Ollama 설치 및 문제 해결 가이드"
description: "AnythingLLM과 Ollama 설정을 위한 빠른 해결책"
---

import { Callout, Tabs } from "nextra/components";
import Image from "next/image";

<Image
  src="/images/faq/ollama-models-not-loading/header-image.png"
  height={1080}
  width={1920}
  quality={100}
  alt="AnythingLLM and Ollama Setup Guide"
/>

# Ollama 연결 문제 해결

## Ollama가 실행 중인지 확인

어떤 수정이나 URL 변경을 시도하기 전에 Ollama가 정상적으로 실행 중인지 확인하세요:

1. 웹 브라우저를 열고 `http://127.0.0.1:11434`로 이동합니다.
2. 다음과 같은 페이지가 표시되어야 합니다:

<Image
  src="/images/faq/ollama-models-not-loading/ollama-running.png"
  height={1080}
  width={1920}
  quality={100}
  alt="Ollama running in background"
/>

이 페이지가 표시되지 않으면 Ollama 설치를 문제 해결하고 정상적으로 실행되고 있는지 확인하세요.

## 자동 URL 감지 (LLM 및 임베딩 제공자)

<Callout type="info" emoji="ℹ️">
  AnythingLLM은 Ollama에 대한 자동 URL 감지 기능을 제공합니다. 자동 감지가 실패할 경우에만 수동 구성이 필요합니다.
</Callout>

### URL이 성공적으로 감지됨

Ollama 제공자를 선택할 때, AnythingLLM은 Ollama URL을 자동으로 감지하려고 시도합니다. 기본 URL 입력 옵션이 숨겨져 있으면 URL이 AnythingLLM에 의해 자동으로 감지된 것입니다.
<Image
  src="/images/faq/ollama-models-not-loading/ollama-detected-collapsed.png"
  height={1080}
  width={1920}
  quality={100}
  style={{
    "border-radius": "20px",
    "margin-top": "20px",
    "object-fit": "none",
    "object-position": "-320px top",
    width: "100%",
    height: "600px",
  }}
  alt="Ollama URL automatically detected"
/>

### URL 감지 실패

수동 엔드포인트 입력이 확장되면 URL을 감지할 수 없었던 것입니다.
<Image
  src="/images/faq/ollama-models-not-loading/ollama-cannot-detect.png"
  height={1080}
  width={1920}
  quality={100}
  style={{
    "border-radius": "20px",
    "margin-top": "20px",
    "object-fit": "none",
    "object-position": "-320px top",
    width: "100%",
    height: "650px",
  }}
  alt="Ollama URL failed detection"
/>

AnythingLLM이 URL을 감지하려고 시도할 때 Ollama가 시작되지 않았다면 Ollama를 시작한 후 `자동 감지` 버튼을 누르세요. 이렇게 하면 URL이 자동으로 감지되고 `모델` 및 `최대 토큰` 값을 선택할 수 있게 됩니다.

## 올바른 Ollama URL 설정

<Callout type="error" emoji="🚨">
  AnythingLLM이 URL을 자동으로 감지하지 못한 경우, 이는 Ollama 설정/구성 문제일 가능성이 높습니다.
</Callout>

Ollama 설치가 정상적으로 실행 중이며 방화벽 등으로 차단되지 않았음을 100% 확인한 경우, URL을 수동으로 설정할 수 있습니다.

자신의 AnythingLLM 버전을 선택하여 올바른 Ollama URL을 확인하세요:

<Tabs items={['데스크톱', '도커', '자가 호스팅']}>
  <Tabs.Tab>
    ### 데스크톱 버전

    사용: `http://127.0.0.1:11434`

    <Image
      src="/images/faq/ollama-models-not-loading/ollama-correct-url.png"
      height={1080}
      width={1920}
      quality={100}
      alt="Correct Ollama Base URL for AnythingLLM Desktop Version"
    />

  </Tabs.Tab>

  <Tabs.Tab>
    ### 도커 버전

    - Windows/macOS: `http://host.docker.internal:11434`
    - Linux: `http://172.17.0.1:11434`
    <Callout type="warning" emoji="⚠️">
      Linux에서는 `host.docker.internal`이 작동하지 않으므로 `http://172.17.0.1:11434`를 사용하세요.
    </Callout>
    <Image
      src="/images/faq/ollama-models-not-loading/ollama-correct-url-docker.png"
      height={1080}
      width={1920}
      quality={100}
      style={{
        "border-radius": "20px",
        "margin-top": "20px",
        "object-fit": "none",
        "object-position": "-320px top",
        "width": "100%",
        "height": "650px",
      }}
      alt="Correct Ollama Base URL for AnythingLLM Docker Version"
    />

  </Tabs.Tab>

  <Tabs.Tab>
    ### 자가 호스팅 버전

    다음 중 하나를 사용:
    - `http://localhost:11434`
    - `http://127.0.0.1:11434`

  </Tabs.Tab>
</Tabs>

## AnythingLLM 데스크톱: 내장 vs. 독립 실행형 Ollama

AnythingLLM 데스크톱은 두 가지 Ollama 옵션을 제공합니다:

1. **내장된 AnythingLLM LLM 제공자**:

   - 별도의 Ollama 인스턴스를 내부적으로 실행합니다.
   - 독립 실행형 Ollama에 다운로드된 모델은 여기에 표시되지 않습니다.

2. **독립 실행형 Ollama**:
   - 시스템에서 Ollama를 별도로 실행합니다.
   - URL `http://127.0.0.1:11434`를 사용합니다.

<Image
  src="/images/faq/ollama-models-not-loading/anythingllm-ollama-provider.png"
  height={1080}
  width={1920}
  quality={100}
  alt="AnythingLLM built-in Ollama provider"
/>

## 문제 해결

여전히 문제가 발생하는 경우:

1. 설정에 맞는 올바른 URL을 사용 중인지 확인하세요.
2. 연결을 차단하는 방화벽이나 네트워크 문제를 확인하세요.
3. Ollama와 AnythingLLM을 모두 재시작하세요.

<Callout type="info" emoji="💡">
  이러한 단계를 시도한 후에도 문제가 지속되면 Discord를 방문하여 질문을 해 주세요.
</Callout>
